# -Generating-Clarifying-Questions-in-Conversational-Search-over-QA-Database
## construct dataset for annotation and run the IIR algorithm
``` 
  python clarifying.py -iir
```
## run the GPT3 model
```
 python use_gpt3.py
```
## run the T5 model
### Please use run_T5.ipynb
## run sentence similarity evaluation
```
  python clarifying.py -s
```
## process human evaluation results and perfrom ranking-based evaluation
```
  python claifying.py -r
```
## The human annotation file
The annotation file is named Batch_rate.csv, which is annotated on the filtered Batch_test.csv dataset.  
Each example has three annotations annotated by different annotators.  
Each row corresponds to one annotation.  
The column "Input.clarification_question" is the clarifying question generated by our model.  
If the annotator thinks the clarifying question is a good one, there will be a TRUE value in the "Answer.good clarifying question.on"	column, and there will be a FALSE value in the "Answer.not good.on" column, vice versa  
If the annotator, played as the patient, answers "yes" to the clarifying question, there will be a TRUE value in the "column.Answer.yes.on"	column, and there will be a FALSE value in the "Answer.no.on"  column, vice versa
